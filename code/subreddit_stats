#!/bin/bash

get_stats()
{
	#file holding data scraped from webpage
	dump_file="subreddit_stats.txt"

	#scrapes the webpage and dumps the data into a .txt file
	#curl transfers data to/from a network server
	#-o writes the output to a file rather than stdout (the command line)
	#-s tells curl to run in silent mode (doesn't display progress meter/errors)
	#-A tells curl to make the request using the following user agent
	#a user agent is essentially ID for the web browser you're using to access the webpage (used for tracking)
	curl -A "JamesKSubstats/.01 (linux)" -s -o $dump_file $subreddit_url

	#opens the dump file, pattern matches with a regular expression, & finds the sub count and active user count
	cat subreddit_stats.txt | sed -e 's/\"//g' -e 's/, /\n/g' | grep subscribers > stats
	cat subreddit_stats.txt | sed -e 's/\"//g' -e 's/, /\n/g' | grep accounts_active: >> stats
	cat subreddit_stats.txt | sed -e 's/\"//g' -e 's/, /\n/g' | grep public_description: >> stats
	cat stats
}

#prompts user for subreddit
echo -n "Enter subreddit name (e.g. dankmemes): "
read subreddit

#appends name of subreddit to reddit url (specifically the about.json file for the subreddit)
reddit_url="https://www.reddit.com/r/"
subreddit_url="$reddit_url$subreddit/about.json"

#checks if subreddit exists. If not, prompts user for new subreddit
if [[ $(wget $subreddit_url -O-) ]] 2>/dev/null
then
	get_stats
else
	echo "The webpage does not exist"
fi
